{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Empathy classifier (RoBERTa - finetuned on our data, prep )",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePwzeHuNh919"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2tokZqttmTA"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers tokenizers\n",
        "!pip install pytorch_lightning==1.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqRRWe4UuuIh"
      },
      "source": [
        "#package imports \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "from typing import List\n",
        "import logging\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "from functools import lru_cache\n",
        "from argparse import Namespace\n",
        "from packaging import version\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI0r_TmmzOsO"
      },
      "source": [
        "#we define the paths for train, val, test (change if desired to match location of splits created in baseline notebook)\n",
        "train_path = \"drive/MyDrive/empathy_classifier_data/my_train_prep.txt\"\n",
        "test_path = \"drive/MyDrive/empathy_classifier_data/my_test_prep.txt\"\n",
        "val_path = \"drive/MyDrive/empathy_classifier_data/my_val_prep.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3EjfPCrz-eH"
      },
      "source": [
        "#create a dictionary which associates each string label to an integer value\n",
        "labels = [ \"no\", \"weak\", \"strong\"]\n",
        "label2int = dict(zip(labels, list(range(len(labels)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_whSBDujRiga"
      },
      "source": [
        "###Now we can start building a classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPbTd5lmuzQn"
      },
      "source": [
        "#we use RoBERTa base model\n",
        "\n",
        "#load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG5lOOAsVgvM"
      },
      "source": [
        "#load actual model\n",
        "model = AutoModelWithLMHead.from_pretrained('roberta-base')\n",
        "base_model = model.base_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO3uhWnx5Jlt"
      },
      "source": [
        "#now we need a custom classification head on top of the LM\n",
        "\n",
        "#note: the following code is partly adapted from Marcin Zablocki's tutorial 'custom classifier on top of bert-like language model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSUMm4Oq7nvR"
      },
      "source": [
        "Use Mish activiation function as it's the one proposed in the original tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCEDXLxq628O"
      },
      "source": [
        "#using Mish\n",
        "@torch.jit.script\n",
        "def mish(input):\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "  \n",
        "class Mish(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return mish(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VDRSRsc71H2"
      },
      "source": [
        "#define an EmpathyClassificationModel class to do the actual fine-tuning\n",
        "\n",
        "class EmpathyClassificationModel(nn.Module):\n",
        "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base_model_output_size, base_model_output_size),\n",
        "            Mish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base_model_output_size, n_classes)\n",
        "        )\n",
        "        \n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if layer.bias is not None:\n",
        "                    layer.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input_, *args):\n",
        "        X, attention_mask = input_\n",
        "        hidden_states = self.base_model(X, attention_mask=attention_mask)\n",
        "        \n",
        "        return self.classifier(hidden_states[0][:, 0, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-N7WSY7Cb7v"
      },
      "source": [
        "Now some dataset preparation for the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDWkjaLV-5tj"
      },
      "source": [
        "!mkdir -p tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMMm5Ye1Db-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5bcaa41-fda9-442a-f4ce-3796d0ab3fae"
      },
      "source": [
        "#load pretrained tokenizer information\n",
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/vocab.json',\n",
              " 'tokenizer/merges.txt',\n",
              " 'tokenizer/added_tokens.json',\n",
              " 'tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FVtbmrzDkF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f16d18-3d41-493a-f1c7-641edf05869f"
      },
      "source": [
        "!ls tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "merges.txt\t\t tokenizer_config.json\tvocab.json\n",
            "special_tokens_map.json  tokenizer.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCLBZsMDn4s"
      },
      "source": [
        "#implementation of CollateFN to do tokenization and batches of sequences\n",
        "\n",
        "class TokenizersCollateFn:\n",
        "    def __init__(self, max_tokens=512): \n",
        "\n",
        "        #RoBERTa uses the BPE tokenizer, similarly to GPT-2\n",
        "        t = ByteLevelBPETokenizer(\n",
        "            \"tokenizer/vocab.json\",\n",
        "            \"tokenizer/merges.txt\"\n",
        "        )\n",
        "        t._tokenizer.post_processor = BertProcessing(\n",
        "            (\"</s>\", t.token_to_id(\"</s>\")),\n",
        "            (\"<s>\", t.token_to_id(\"<s>\")),\n",
        "        )\n",
        "        t.enable_truncation(max_tokens)\n",
        "        t.enable_padding(pad_id=t.token_to_id(\"<pad>\"))\n",
        "        self.tokenizer = t\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
        "        sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
        "        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
        "        labels = torch.tensor([x[1] for x in batch])\n",
        "        \n",
        "        return (sequences_padded, attention_masks_padded), labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktr6xeMuISin"
      },
      "source": [
        "#class to create dataset objects from the data\n",
        "\n",
        "class EmpathyDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        super().__init__()\n",
        "        self.data_column = \"text\"\n",
        "        self.class_column = \"class\"\n",
        "        self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
        "                               engine=\"python\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGWw4wGEJGhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cc64ab-b770-497f-cfc3-0c2b3424597b"
      },
      "source": [
        "#sanity check, visualise one sample and label (converted to numerical)\n",
        "ds = EmpathyDataset(train_path)\n",
        "ds[20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('would you say that someone else caused you to feel this way', 1)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJHhNRcZK7sV"
      },
      "source": [
        "#we use PyTorch Lighning for training. Lightning methods are defined here\n",
        "\n",
        "class TrainingModule(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.model = EmpathyClassificationModel(AutoModelWithLMHead.from_pretrained(\"roberta-base\").base_model, len(labels))\n",
        "        self.loss = nn.CrossEntropyLoss() #cross entropy loss since this is multi-class classification\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "    def step(self, batch, step_name=\"train\"):\n",
        "        X, y = batch\n",
        "        loss = self.loss(self.forward(X), y)\n",
        "        loss_key = f\"{step_name}_loss\"\n",
        "        tensorboard_logs = {loss_key: loss}\n",
        "\n",
        "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
        "               \"progress_bar\": {loss_key: loss}}\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        return self.model(X, *args)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.step(batch, \"train\")\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.step(batch, \"val\")\n",
        "\n",
        "    def validation_end(self, outputs: List[dict]):\n",
        "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        return {\"val_loss\": loss}\n",
        "        \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.step(batch, \"test\")\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.create_data_loader(self.hparams.val_path)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.create_data_loader(self.hparams.test_path)\n",
        "                \n",
        "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
        "        return DataLoader(\n",
        "                    EmpathyDataset(ds_path),\n",
        "                    batch_size=self.hparams.batch_size,\n",
        "                    num_workers=4,\n",
        "                    shuffle=shuffle,\n",
        "                    collate_fn=TokenizersCollateFn()\n",
        "        )\n",
        "        \n",
        "    @lru_cache()\n",
        "    def total_steps(self):\n",
        "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr) #we use AdamW as this usually performs well\n",
        "        lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                    optimizer,\n",
        "                    num_warmup_steps=self.hparams.warmup_steps,\n",
        "                    num_training_steps=self.total_steps(),\n",
        "        )\n",
        "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3FiLr3LBrjs"
      },
      "source": [
        "#chosen hyperparams:\n",
        "hparams = Namespace(\n",
        "    train_path=train_path,\n",
        "    val_path=val_path,\n",
        "    test_path=test_path,\n",
        "    batch_size=16,\n",
        "    warmup_steps=100,\n",
        "    epochs=20,\n",
        "    lr=1.35E-05,\n",
        "    accumulate_grad_batches=1\n",
        ")\n",
        "module = TrainingModule(hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8Jv_U25B37g"
      },
      "source": [
        "#rubbish collection\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk4OrTTm1WZD"
      },
      "source": [
        "####Now we can fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRnl4HXvB5-T"
      },
      "source": [
        "#train\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
        "                     accumulate_grad_batches=hparams.accumulate_grad_batches)\n",
        "\n",
        "trainer.fit(module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bs2Y7aky4QS"
      },
      "source": [
        "##Evaluation on the held-out test\n",
        "This is a multi-class classification task for which we used cross-entropy loss. We will evaluate using accuracy, precision, recall, F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrAoUr9r2CYs",
        "outputId": "0080424f-e116-467e-d989-538b496e6a18"
      },
      "source": [
        "with torch.no_grad():\n",
        "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
        "    module.eval().cuda()\n",
        "    true_y, pred_y = [], []\n",
        "    for i, batch_ in enumerate(module.test_dataloader()):\n",
        "        (X, attn), y = batch_\n",
        "        batch = (X.cuda(), attn.cuda())\n",
        "        print(progress[i % len(progress)], end=\"\\r\")\n",
        "        y_pred = torch.argmax(module(batch), dim=1)\n",
        "        true_y.extend(y.cpu())\n",
        "        pred_y.extend(y_pred.cpu())\n",
        "print(\"\\n\" + \"_\" * 80)\n",
        "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\r-\r\\\r|\r/\r-\r\\\r\n",
            "________________________________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no     0.8889    0.7742    0.8276        31\n",
            "        weak     0.6200    0.7561    0.6813        41\n",
            "      strong     0.7941    0.6923    0.7397        39\n",
            "\n",
            "    accuracy                         0.7387       111\n",
            "   macro avg     0.7677    0.7409    0.7495       111\n",
            "weighted avg     0.7563    0.7387    0.7427       111\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "JGIQPsgl77xJ",
        "outputId": "7d72660c-cf4c-4641-f424-4ce7a422190b"
      },
      "source": [
        "#plot confusion matrix\n",
        "\n",
        "cm = confusion_matrix(true_y, pred_y, labels=range(len(labels)))\n",
        "df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
        "\n",
        "plt.rcParams.update({'font.size': 12}) \n",
        "plt.figure(figsize = (10,8));\n",
        "sn.heatmap(df_cm, annot=True, cmap='Greens', fmt='g');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}